{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time, subprocess\n",
    "from collections import defaultdict as ddict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pdb\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.models import build_or_load_gen_model\n",
    "from evaluator import smooth_bleu\n",
    "from evaluator.CodeBLEU import calc_code_bleu\n",
    "from evaluator.bleu import _bleu\n",
    "from utils.utils import *\n",
    "from utils.metrics import *\n",
    "# from utils.configs import *\n",
    "from utils.replay import Buffer\n",
    "from models.T5prompt import PromptTuneT5\n",
    "from dataloaders.generation_loader import CodeXGlueDataModule\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UTILITY FUNCTIONS FOR ARGUMENTS\n",
    "\n",
    "def add_args(parser):\n",
    "    parser.add_argument(\"--task\", type=str, default=None,)\n",
    "    parser.add_argument(\"--sub_task\", type=str, default='')\n",
    "    parser.add_argument(\"--stream\", type=str, default=None)\n",
    "\n",
    "    parser.add_argument(\"--lang\", type=str, default='')\n",
    "    parser.add_argument(\"--eval_task\", type=str, default='')\n",
    "    parser.add_argument(\"--model_type\", default=\"codet5\", type=str, choices=['roberta', 'bart', 'codet5'])\n",
    "    parser.add_argument(\"--add_lang_ids\", action='store_true')\n",
    "    parser.add_argument(\"--data_num\", default=-1, type=int)\n",
    "    parser.add_argument(\"--bleu_samples\", default=5000, type=int)\n",
    "\n",
    "    # task specific params\n",
    "    parser.add_argument('--patience', nargs='+', default=None, type=int, help='Patience for early stopping.')\n",
    "    parser.add_argument('--num_train_epochs', nargs='+', default=None, type=int, help='Number of epochs')\n",
    "    parser.add_argument('--learning_rate', nargs='+', default=None, type=float, help='Learning rate')\n",
    "    parser.add_argument('--max_source_length', nargs='+', default=None, type=int, help='max src len')\n",
    "    parser.add_argument('--max_target_length', nargs='+', default=None, type=int, help='max tgt len')\n",
    "    parser.add_argument('--train_batch_size', nargs='+', default=None, type=int, help='Batch size per GPU/CPU for training.')\n",
    "    parser.add_argument('--eval_batch_size', nargs='+', default=None, type=int, help='Batch size per GPU/CPU for evaluation.')\n",
    "\n",
    "    ## replay params\n",
    "    parser.add_argument(\"--replay\", default='res', type=str)\n",
    "    parser.add_argument(\"--buffer_size\", default=0, type=int)\n",
    "    parser.add_argument(\"--buffer_bs\", default=8, type=int)\n",
    "    parser.add_argument(\"--replay_epoch_end\", action='store_true')\n",
    "    parser.add_argument(\"--alpha\", default=0, type=float, help=\"Mixing weight for buffer loss.\")\n",
    "\n",
    "    ## Prompting params\n",
    "    parser.add_argument(\"--pool_lambda\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--prompt_loss_type\", default='basic', type=str)\n",
    "    parser.add_argument(\"--num_prompts_per_task\", default=0, type=int)\n",
    "    parser.add_argument(\"--prompt_init\", default='vocab', type=str)\n",
    "    parser.add_argument(\"--prompt_key_init\", default='uniform', type=str)\n",
    "    parser.add_argument(\"--prompt_lr\", default=10, type=float)\n",
    "    parser.add_argument(\"--query_key_lr\", default=10, type=float)\n",
    "    parser.add_argument(\"--query_pooling_mode\", default='mean', type=str)\n",
    "    parser.add_argument(\"--train_only_prompts\", action='store_true')\n",
    "    parser.add_argument(\"--io_queries\", action='store_true')\n",
    "    parser.add_argument(\"--prompt_pool\", action='store_true')\n",
    "    parser.add_argument(\"--batched_prompts\", action='store_true')\n",
    "    parser.add_argument(\"--pool_freq_norm\", action='store_true')\n",
    "    parser.add_argument(\"--pool_freq\", action='store_true')\n",
    "    parser.add_argument(\"--compute_avg_sim\", action='store_true')\n",
    "    parser.add_argument(\"--pool_size\", default=60, type=int)\n",
    "    parser.add_argument(\"--num_pool_prompt_tokens\", default=5, type=int)\n",
    "    parser.add_argument(\"--uniform_scale\", default=1, type=float)\n",
    "    parser.add_argument(\"--prompt_projection\", action='store_true')\n",
    "    parser.add_argument(\"--separate_projection\", action='store_true')\n",
    "    parser.add_argument(\"--projection_hid_dim\", default=512, type=int)\n",
    "    parser.add_argument(\"--projection_out_dim\", default=512, type=int)\n",
    "    parser.add_argument(\"--dropout\", default=0.1, type=float)\n",
    "    parser.add_argument(\"--projection_plot\", default='', type=str)\n",
    "\n",
    "\n",
    "    ## Directories.\n",
    "    parser.add_argument(\"--project_dir\", type=str, default='/mnt/efs/people/ptky/project/incremental-learning',)\n",
    "    parser.add_argument(\"--data_dir\", type=str, default='data',)\n",
    "    parser.add_argument(\"--output_dir\", default='saved_runs', type=str,\n",
    "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "    parser.add_argument(\"--cache_path\", type=str, default='data/cache',)\n",
    "    parser.add_argument(\"--summary_dir\", type=str, default='saved_runs/logs',)\n",
    "    parser.add_argument(\"--res_dir\", type=str, default='',)\n",
    "    parser.add_argument(\"--res_fn\", type=str, default='')\n",
    "\n",
    "    parser.add_argument(\"--add_task_prefix\", action='store_true', help=\"Whether to add task prefix for t5 and codet5\")\n",
    "    parser.add_argument(\"--save_last_checkpoints\", action='store_true')\n",
    "    parser.add_argument(\"--always_save_model\", action='store_true')\n",
    "    parser.add_argument(\"--calc_stats\", action='store_true')\n",
    "\n",
    "    # wandb params\n",
    "    parser.add_argument(\"--debug\", action='store_true')\n",
    "    parser.add_argument(\"--name\", type=str, default='test')\n",
    "    parser.add_argument(\"--project_name\", type=str, default='debug')\n",
    "\n",
    "\n",
    "    ## Huggingface params.\n",
    "    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n",
    "    parser.add_argument(\"--model_name_or_path\", default=\"Salesforce/codet5-small\", type=str, help=\"Path to pre-trained model: e.g. Salesforce/codet5-small\")\n",
    "    parser.add_argument(\"--tokenizer_name\", default=\"Salesforce/codet5-small\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "    parser.add_argument(\"--load_model_path\", default=None, type=str, help=\"Path to trained model: Should contain the .bin files\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--train_filename\", default=None, type=str,\n",
    "                        help=\"The train filename. Should contain the .jsonl files for this task.\")\n",
    "    parser.add_argument(\"--dev_filename\", default=None, type=str,\n",
    "                        help=\"The dev filename. Should contain the .jsonl files for this task.\")\n",
    "    parser.add_argument(\"--test_filename\", default=None, type=str,\n",
    "                        help=\"The test filename. Should contain the .jsonl files for this task.\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--no_train\", action='store_true', help=\"Whether to run eval on the train set.\")\n",
    "    parser.add_argument(\"--no_eval\", action='store_true', help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\"--no_eval_bleu\", action='store_true', help=\"Whether to evaluate bleu on dev set.\")\n",
    "    parser.add_argument(\"--no_eval_all\", action='store_true', help=\"Whether to run eval on all tasks dev set after each task.\")\n",
    "    parser.add_argument(\"--no_test\", action='store_true', help=\"Whether to evaluate on test set.\")\n",
    "    parser.add_argument(\"--full_matrix_eval\", action='store_true', help=\"evaluate on future tasks as well in each epoch.\")\n",
    "    parser.add_argument(\"--zeroshot\", action='store_true', help=\"Evaluate zeroshot performance on the test set.\")\n",
    "    parser.add_argument(\"--do_lower_case\", action='store_true', help=\"Set this flag if you are using an uncased model.\")\n",
    "    parser.add_argument(\"--no_cuda\", action='store_true', help=\"Avoid using CUDA when available\")\n",
    "\n",
    "    parser.add_argument(\"--adafactor\", action='store_true', help=\"Use adafactor instead of AdamW\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--beam_size\", default=5, type=int, help=\"beam size for beam search\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "\n",
    "    parser.add_argument(\"--num_workers\", default=os.cpu_count(), type=int, )\n",
    "    parser.add_argument(\"--pin_memory\", default=True, type=bool, )\n",
    "    parser.add_argument(\"--save_steps\", default=-1, type=int, )\n",
    "    parser.add_argument(\"--num_saves\", default=10, type=int, )\n",
    "    parser.add_argument(\"--log_steps\", default=-1, type=int, )\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument(\"--eval_steps\", default=-1, type=int, help=\"\")\n",
    "    parser.add_argument(\"--train_steps\", default=-1, type=int, help=\"\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=100, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument('--seed', type=int, default=1234, help=\"random seed for initialization\")\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "\n",
    "    if args.task in ['summarize']:\n",
    "        args.lang = args.sub_task\n",
    "    elif args.task in ['refine', 'concode', 'clone']:\n",
    "        args.lang = 'java'\n",
    "    elif args.task == 'defect':\n",
    "        args.lang = 'c'\n",
    "    elif args.task == 'translate':\n",
    "        args.lang = 'c_sharp' if args.sub_task == 'java-cs' else 'java'\n",
    "\n",
    "    args.project_name = f\"aws-{args.project_name}\"\n",
    "    args.cpu_cont = multiprocessing.cpu_count()\n",
    "\n",
    "    args.data_dir = os.path.join(args.project_dir, args.data_dir)\n",
    "    args.cache_path = os.path.join(args.project_dir, args.cache_path)\n",
    "    args.output_dir = os.path.join(args.project_dir, args.output_dir)\n",
    "    os.makedirs(args.cache_path, exist_ok=True)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    if not args.compute_avg_sim:\n",
    "        while True:\n",
    "            args.run_output_dir = f\"{args.output_dir}/{args.project_name}/{args.name}~try={i}\"\n",
    "            if not os.path.exists(args.run_output_dir):\n",
    "                os.makedirs(args.run_output_dir)\n",
    "                args.name = args.name + f\"~try={i}\"\n",
    "                break\n",
    "            i += 1\n",
    "    else:\n",
    "        args.run_output_dir = f\"{args.output_dir}/{args.project_name}/{args.name}\"\n",
    "\n",
    "    args.log_dir = os.path.join(args.run_output_dir, 'logs')\n",
    "    args.checkpoint_dir = os.path.join(args.run_output_dir, 'checkpoints')\n",
    "    os.makedirs(args.log_dir, exist_ok=True)\n",
    "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def set_dist(args):\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        # Setup for distributed data parallel\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.n_gpu = 1\n",
    "    cpu_cont = multiprocessing.cpu_count()\n",
    "    args.device = device\n",
    "    args.cpu_cont = cpu_cont\n",
    "\n",
    "def get_task_arglist(args, keys, datamodule):\n",
    "    for key in keys:\n",
    "        if hasattr(args, key):\n",
    "            att_value = getattr(args, key)\n",
    "            new_att_value = []\n",
    "        else:\n",
    "            raise ValueError(f\"Key {key} is not in args!\")\n",
    "\n",
    "        if att_value is None:\n",
    "            for task in datamodule.all_tasks:\n",
    "                key_map = {\"num_train_epochs\": 'epoch', 'learning_rate': 'lr', 'patience': 'patience', 'max_source_length':'src_len',\n",
    "                            'max_target_length':'trg_len', 'train_batch_size':'tbs', 'eval_batch_size':'ebs'}\n",
    "                task_val = datamodule.task_params[task][key_map[key]]\n",
    "                new_att_value.append(task_val)\n",
    "        elif len(att_value) == 1:\n",
    "            new_att_value = att_value * len(datamodule.all_tasks)\n",
    "        elif len(att_value) == len(datamodule.all_tasks):\n",
    "            new_att_value = att_value\n",
    "        setattr(args, key, new_att_value)\n",
    "    return args\n",
    "\n",
    "def set_seed(args):\n",
    "    \"\"\"set random seed.\"\"\"\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, filepath, criteria):\n",
    "    file = os.path.join(filepath, f'checkpoints/{criteria}.bin')\n",
    "    \n",
    "    if os.path.exists(file):\n",
    "        loaded_state_dict = torch.load(file)\n",
    "        for buf in model._buffers.keys():\n",
    "            try:\n",
    "                model._buffers[buf] = torch.empty_like(loaded_state_dict[buf])\n",
    "            except:\n",
    "                print(f\"{buf} not found in stored model.\")\n",
    "        model.load_state_dict(loaded_state_dict, strict=False)\n",
    "        print(f'LOADED MODEL WEIGHT FROM FILE: {file}')\n",
    "        return model\n",
    "    else:\n",
    "        print('This file doesnt exist')\n",
    "\n",
    "def get_task_queries(args, dataloader, model, tokenizer, eval_task=None, phase='eval'):\n",
    "    model.eval()\n",
    "    task_sims = []\n",
    "    for batch in tqdm(dataloader, total=len(dataloader), desc=\"Eval ppl\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        source_ids, target_ids = batch\n",
    "        targets = target_ids if args.io_queries else None\n",
    "        # pdb.set_trace()\n",
    "        source_mask = source_ids.ne(tokenizer.pad_token_id).type(torch.float)\n",
    "        with torch.no_grad():\n",
    "            task_id = model.get_task_id(eval_task)\n",
    "            queries = model.get_inference_stats(input_ids=source_ids, labels=targets, task_id=task_id, phase=phase)\n",
    "            queries = torch.nn.functional.normalize(queries, p=2.0, dim=1)\n",
    "            task_sims.append(queries.detach().cpu())\n",
    "    return torch.vstack(task_sims)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INITIALIZE ARGPARSE AND SET PARAMETERS\n",
    "\n",
    "t0 = time.time()\n",
    "parser = argparse.ArgumentParser()\n",
    "args = add_args(parser)\n",
    "\n",
    "set_dist(args)\n",
    "seed_everything(args.seed)\n",
    "\n",
    "args.stream = 'concode_none,translate_java-cs,summarize_ruby,refine_small'\n",
    "# args.stream = 'concode_none,translate_java-cs,summarize_ruby'\n",
    "args.prompt_method = 'pool'\n",
    "args.pool_freq = True\n",
    "# args.pool_freq_norm = True\n",
    "# args.num_prompts_per_task = 20\n",
    "args.eval_batch_size = [80]\n",
    "args.query_pooling_mode = 'mean'\n",
    "args.data_num = 5000\n",
    "args.debug = True\n",
    "# args.no_keys = True\n",
    "# args.batched_prompts = True\n",
    "args.pool_size = 500\n",
    "args.num_pool_prompt_tokens = 1\n",
    "args.num_prompts_per_task = 100\n",
    "\n",
    "key_map = {\n",
    "    \"curr_count\": \"All\",\n",
    "    \"concode_none_count\": \"CodeGen\",\n",
    "    \"translate_java-cs_count\": \"CodeTrans\",\n",
    "    \"summarize_ruby_count\": \"CodeSumm\",\n",
    "    \"refine_small_count\": \"CodeRef\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZE DATALOADER AND MODEL\n",
    "\n",
    "config, model, tokenizer = build_or_load_gen_model(args)\n",
    "datamodule = CodeXGlueDataModule(args, tokenizer)\n",
    "all_tasks = datamodule.all_tasks\n",
    "\n",
    "task_specific_params = ['num_train_epochs', 'learning_rate', 'patience', 'max_source_length',\n",
    "                        'max_target_length', 'train_batch_size', 'eval_batch_size']\n",
    "args = get_task_arglist(args, task_specific_params, datamodule)\n",
    "datamodule.setup(stage='fit')\n",
    "train_dataloaders = datamodule.train_dataloader()\n",
    "\n",
    "if args.num_prompts_per_task > 0 or args.prompt_method == 'pool':\n",
    "    model = PromptTuneT5(args, model, tokenizer, datamodule)\n",
    "if args.prompt_method == 'pool':\n",
    "    model.initialize_prompt_pool(load=True)\n",
    "model.to(args.device)\n",
    "print('INITIALIZED DATALOADER AND MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(model, key_map):\n",
    "    counts = {}\n",
    "    for k,v in model._buffers.items():\n",
    "        if 'count' in k:\n",
    "            counts[key_map[k]] = v.type(torch.int).cpu().numpy()\n",
    "    return counts\n",
    "\n",
    "def get_pairwise_intersection(data, noise_perc_thresh, noise_exact_thresh=10):\n",
    "    tasks = list(data.keys())\n",
    "    tasks.remove('All')\n",
    "    print(tasks)\n",
    "    \n",
    "    common_union = np.zeros((len(tasks), len(tasks)))\n",
    "    common_all = np.zeros((len(tasks), len(tasks)))\n",
    "\n",
    "    for i, task_i in enumerate(tasks):\n",
    "        for j, task_j in enumerate(tasks):\n",
    "            arr_i = np.array(data[task_i])\n",
    "            arr_j = np.array(data[task_j])\n",
    "            thresh_i = min(int(arr_i.sum() * noise_perc_thresh), noise_exact_thresh)\n",
    "            thresh_j = min(int(arr_j.sum() * noise_perc_thresh), noise_exact_thresh)\n",
    "            used_i = np.where(arr_i > thresh_i)[0]\n",
    "            used_j = np.where(arr_j>thresh_j)[0]\n",
    "            intersect_over_union = len(np.intersect1d(used_i, used_j)) / len(np.union1d(used_i, used_j))\n",
    "            intersect_over_all = len(np.intersect1d(used_i, used_j)) / len(arr_i)\n",
    "            common_union[i,j] = intersect_over_union\n",
    "            common_all[i,j] = intersect_over_all\n",
    "    return common_union, common_all\n",
    "\n",
    "def get_prompt_num_task_participation(data, noise_perc_thresh, noise_exact_thresh=10):\n",
    "    tasks = list(data.keys())\n",
    "    tasks.remove('All')\n",
    "    print(tasks)\n",
    "\n",
    "    prompt_tasks = np.zeros(len(data[tasks[0]]), dtype=int)\n",
    "    for task_id, task_name in enumerate(tasks):\n",
    "        task_counts = data[task_name]\n",
    "        thresh_i = min(int(task_counts.sum() * noise_perc_thresh), noise_exact_thresh)\n",
    "        thresh_j = min(int(task_counts.sum() * noise_perc_thresh), noise_exact_thresh)\n",
    "        thresh = min(thresh_i, thresh_j)\n",
    "        for pid, pcount in enumerate(task_counts):\n",
    "            if pcount > thresh:\n",
    "                prompt_tasks[pid] += 1\n",
    "    num_keys_in_tasks = np.zeros(len(tasks)+1, dtype=int)\n",
    "    for i in range(len(tasks)+1):\n",
    "        num_keys_in_tasks[i] = len(np.where(prompt_tasks == i)[0])\n",
    "    assert sum(num_keys_in_tasks) == len(prompt_tasks)\n",
    "    return prompt_tasks, num_keys_in_tasks\n",
    "\n",
    "# ### GET INITIAL FREQUENCY COUNT STATISTICS \n",
    "# run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-tune_lr/pool100_data_keyaggrandom_qlr0.1~try=0'\n",
    "# model = load_checkpoint(model, run_path, 'best-bleu')\n",
    "# training_counts = get_counts(model, key_map)\n",
    "# prompt_tasks, num_keys_in_tasks = get_prompt_num_task_participation(training_counts, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### GET QUERIES FOR ALL MODEL CHECKPOINTS\n",
    "\n",
    "model_types = ['first', 'concode_none', 'translate_java-cs', 'summarize_ruby', 'refine_small', 'best-bleu']\n",
    "\n",
    "criteria_reps, criteria_keys, criteria_targets = [], [], []\n",
    "for criteria in model_types:\n",
    "    # run_path = \"/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool60_data_keyaggrandom~try=0\"\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool60_data_nokeys_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool60_data_freqnorm_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool60_data_batched_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool300_data_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool100_data_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool100_data_batched_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool100_data_freqnorm_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool100_data_nokeys_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool500_data_keyaggrandom~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool100_data_keyaggrandom_ER~try=0'\n",
    "\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-tune_lr/pool100_data_keyaggrandom_qlr0.1~try=0'\n",
    "    # run_path = '/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-tune_plam/pool100_data_keyaggrandom_plam0.001~try=0'\n",
    "    run_path = \"/mnt/efs/people/ptky/project/incremental-learning/saved_runs/aws-init5k_1/pool100_data_fixed_selection~try=0\"\n",
    "\n",
    "    model = load_checkpoint(model, run_path, criteria)\n",
    "    if criteria == 'best-bleu':\n",
    "        training_counts = get_counts(model, key_map)\n",
    "        prompt_tasks, num_keys_in_tasks = get_prompt_num_task_participation(training_counts, 0.01, 10)\n",
    "        print(f'how many prompts belong to how many tasks!'.upper())\n",
    "        print(num_keys_in_tasks.tolist())\n",
    "\n",
    "    task_reps = ddict(list)\n",
    "    for curr_idx, curr_task in enumerate(all_tasks):\n",
    "        train_loader = train_dataloaders[curr_task]\n",
    "        print(f\"***** Eval results on Task {curr_task} *****\".upper())\n",
    "        task_reps[curr_idx] = get_task_queries(args, train_loader, model, tokenizer, eval_task=curr_task, phase='init')\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    reps, targets = [], []\n",
    "    for k,v in task_reps.items():\n",
    "        reps.append(v)\n",
    "        targets.append(torch.ones(v.shape[0]) * int(k))\n",
    "    reps = torch.vstack(reps)\n",
    "    targets = torch.cat(targets).numpy()\n",
    "    criteria_reps.append(reps)\n",
    "    criteria_targets.append(targets)\n",
    "    criteria_keys.append(model.prompt_keys.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['first', 'concode_none', 'translate_java-cs', 'summarize_ruby', 'refine_small', 'last']\n",
    "for ii, criteria in enumerate(model_types):\n",
    "    args.projection_plot = f'pca-3-{criteria}'\n",
    "    method, dims, name, *_ = args.projection_plot.split('-')\n",
    "    \n",
    "    reps = criteria_reps[ii]\n",
    "    targets = criteria_targets[ii]\n",
    "    keys_ = criteria_keys[ii]\n",
    "    keys = keys_ / np.linalg.norm(keys_, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "    if method =='tsne':\n",
    "        print('Performing t-sne projection')\n",
    "        meth = TSNE(n_components=int(dims), verbose=1, random_state=123)\n",
    "    elif method == 'pca':\n",
    "        print('Performing PCA projection')\n",
    "        meth = PCA(n_components=int(dims))\n",
    "    queries = meth.fit_transform(reps)\n",
    "    keys_trans = meth.transform(keys)\n",
    "    out = np.concatenate([queries, keys_trans])\n",
    "    targets = np.concatenate([targets, np.ones(keys_trans.shape[0]) * (max(targets)+1)])\n",
    "\n",
    "    if int(dims) == 3:\n",
    "\n",
    "        Xax = out[:,0]\n",
    "        Yax = out[:,1]\n",
    "        Zax = out[:,2]\n",
    "\n",
    "        cdict = {0:'orange',1:'darkcyan', 2:'burlywood', 3:'cyan', 4:'red'}\n",
    "        labl = {0:'CodeGen Queries',1:'CodeTrans Queries',2:\"CodeSumm Queries\", 3:'CodeRef Queries', 4:\"Keys\"}\n",
    "        marker = {0:'*',1:'o',2:'v',3:'s',4:'x'}\n",
    "        alpha = {0:0.2,1:0.2,2:0.2,3:0.2,4:0.7}\n",
    "\n",
    "        # cdict = {0:'orange',1:'darkcyan', 2:'burlywood', 3:'red'}\n",
    "        # labl = {0:'CodeGen Queries',1:'CodeTrans Queries',2:\"CodeSumm Queries\", 3:\"Keys\"}\n",
    "        # marker = {0:'*',1:'o',2:'v',3:'x'}\n",
    "        # alpha = {0:0.2,1:0.2,2:0.2,3:0.7}\n",
    "\n",
    "        fig = plt.figure(figsize=(7,5))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        plt.title(f\"Task: {criteria}\")#\\tKeys with {key_init} inititalization\")\n",
    "\n",
    "        fig.patch.set_facecolor('white')\n",
    "        for l in np.unique(targets):\n",
    "            ix=np.where(targets==l)\n",
    "            ax.scatter(Xax[ix], Yax[ix], Zax[ix], c=cdict[l], s=20,\n",
    "                    label=labl[l], marker=marker[l], alpha=alpha[l])\n",
    "        # for loop ends\n",
    "        ax.set_xlabel(\"First PC\", fontsize=14)\n",
    "        ax.set_ylabel(\"Second PC\", fontsize=14)\n",
    "        ax.set_zlabel(\"Third PC\", fontsize=14)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        os.makedirs(f\"{run_path}/plots/\", exist_ok=True)\n",
    "        plt.savefig(f\"{run_path}/plots/{args.projection_plot}.png\")\n",
    "    elif int(dims) == 2:\n",
    "        df = pd.DataFrame()\n",
    "        df[\"y\"] = targets\n",
    "        df[\"First PC\"] = out[:,0]\n",
    "        df[\"Second PC\"] = out[:,1]\n",
    "        sns.scatterplot(x=\"First PC\", y=\"Second PC\", hue=df.y.tolist(),\n",
    "                        palette=sns.color_palette(\"hls\", np.unique(targets).shape[0]),\n",
    "                        data=df).set(title=\"CodeT5 queries for task\")\n",
    "        # fig = plot[0].get_figure()\n",
    "        # plt.show()\n",
    "        plt.savefig(f\"{run_path}/plots/{args.projection_plot}.png\")\n",
    "    else:\n",
    "        raise ValueError(f\"{dims }Dimension not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(arr):\n",
    "    # sim_df = pd.DataFrame(arr, columns=['CodeGen','CodeTrans','CodeSumm','CodeRef'])\n",
    "    sim_df = pd.DataFrame(arr, columns=['CodeGen','CodeTrans','CodeSumm'])\n",
    "    sim_df = sim_df * 100\n",
    "    sim_df = sim_df.round(2)\n",
    "    # sim_df['tasks'] = ['CodeGen','CodeTrans','CodeSumm','CodeRef']\n",
    "    sim_df['tasks'] = ['CodeGen','CodeTrans','CodeSumm']\n",
    "    sim_df.set_index('tasks', inplace=True)\n",
    "    return sim_df\n",
    "\n",
    "def plot_heatmap(data, title, savename=None, vmin=None, vmax=None):\n",
    "\n",
    "    params = {\"lines.linewidth\": 1.1, \"font.size\":14, \"axes.titlesize\":18, \n",
    "            \"axes.labelsize\":20, 'xtick.labelsize':14, 'ytick.labelsize':14,\n",
    "            'legend.fontsize': 12, 'legend.labelspacing': 0.3, 'legend.handletextpad': 0.2,\n",
    "            'legend.borderpad': 0.4}\n",
    "    sns.set(style=\"ticks\", rc=params)\n",
    "\n",
    "    ax = sns.heatmap(data, annot=True, fmt='.1f', linewidths=.5, cmap=\"YlGnBu\", vmin=vmin, vmax=vmax)\n",
    "    ax.set(ylabel=r'', xlabel=r'')\n",
    "    ax.set_title(f'{title}')\n",
    "    ax.set_aspect('auto')\n",
    "    if savename is not None:\n",
    "        plt.savefig(f'./figures/{savename}.pdf', dpi=1000,  bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "def get_intersection_stats(data, noise_perc_thresh, noise_exact_thresh=20):\n",
    "    # tasks = ['CodeGen','CodeTrans','CodeSumm','CodeRef']\n",
    "    tasks = ['CodeGen','CodeTrans','CodeSumm']\n",
    "    common_union = np.zeros((len(tasks), len(tasks)))\n",
    "    common_all = np.zeros((len(tasks), len(tasks)))\n",
    "\n",
    "    for i, task_i in enumerate(tasks):\n",
    "        for j, task_j in enumerate(tasks):\n",
    "            arr_i = np.array(data[task_i])\n",
    "            arr_j = np.array(data[task_j])\n",
    "            thresh_i = min(int(arr_i.sum() * noise_perc_thresh), noise_exact_thresh)\n",
    "            thresh_j = min(int(arr_j.sum() * noise_perc_thresh), noise_exact_thresh)\n",
    "            # print(arr_i.sum(), arr_j.sum())\n",
    "            used_i = np.where(arr_i > thresh_i)[0]\n",
    "            used_j = np.where(arr_j>thresh_j)[0]\n",
    "            intersect_over_union = len(np.intersect1d(used_i, used_j)) / len(np.union1d(used_i, used_j))\n",
    "            intersect_over_all = len(np.intersect1d(used_i, used_j)) / len(arr_i)\n",
    "            common_union[i,j] = intersect_over_union\n",
    "            common_all[i,j] = intersect_over_all\n",
    "    return common_union, common_all\n",
    "\n",
    "def get_count_df(counts, noise_perc_thresh, noise_exact_thresh):\n",
    "    (sparse_freq_overlap, dense_freq_overlap) = get_intersection_stats(counts, noise_perc_thresh, noise_exact_thresh)\n",
    "    sparse_freq_overlap_df = get_df(sparse_freq_overlap)\n",
    "    dense_freq_overlap_df = get_df(dense_freq_overlap)\n",
    "    return sparse_freq_overlap, dense_freq_overlap\n",
    "\n",
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=(\"black\", \"white\"),\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {}\n",
    "# tasks = ['CodeGen','CodeTrans','CodeSumm','CodeRef']\n",
    "tasks = ['CodeGen','CodeTrans','CodeSumm']\n",
    "\n",
    "\n",
    "print(f\"'Prompt_id' : list(range(60)),\")\n",
    "for k,v in model._buffers.items():\n",
    "    if 'count' in k:\n",
    "        print(f\"'{key_map[k]}': {v.type(torch.int).tolist()},\")\n",
    "        count_dict[key_map[k]] = v.type(torch.int).tolist()\n",
    "count_dict['prompt_id'] = range(len(count_dict['All']))\n",
    "\n",
    "sparse_pool , dense_pool  = get_count_df(count_dict , noise_perc_thresh=0.01, noise_exact_thresh=4)\n",
    "fig, ax = plt.subplots()\n",
    "im, cbar = heatmap(dense_pool, tasks, tasks, ax=ax,\n",
    "                   cmap=\"YlGn\", cbarlabel=\"Prompt Overlap\")\n",
    "texts = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "plt.savefig(f\"{run_path}/plots/overlap.png\")\n",
    "\n",
    "# plot_heatmap(dense_pool, title=\"Training Counts\", savename=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('WinKawaks/vit-small-patch16-224')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22M'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "model_size = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "\"{}M\".format(round(model_size / 1e+6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fef4faef6beebd906a305a061bdc2b876a12dc48da7e6144a498faaddef1a92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
